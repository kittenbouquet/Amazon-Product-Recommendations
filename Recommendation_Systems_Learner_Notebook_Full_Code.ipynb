{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXQzH0nC5JtP"
   },
   "source": [
    "# **Project: Amazon Product Recommendation System**\n",
    "\n",
    "\n",
    "\n",
    "Welcome to the project on Recommendation Systems. We will work with the Amazon product reviews dataset for this project. The dataset contains ratings of different electronic products. It does not include information about the products or reviews to avoid bias while building the model. \n",
    "\n",
    "--------------\n",
    "## **Context:**\n",
    "--------------\n",
    "\n",
    "Today, information is growing exponentially with volume, velocity and variety throughout the globe. This has lead to information overload, and too many choices for the consumer of any business. It represents a real dilemma for these consumers and they often turn to denial. Recommender Systems are one of the best tools that help recommending products to consumers while they are browsing online. Providing personalized recommendations which is most relevant for the user is what's most likely to keep them engaged and help business. \n",
    "\n",
    "E-commerce websites like Amazon, Walmart, Target and Etsy use different recommendation models to provide personalized suggestions to different users. These companies spend millions of dollars to come up with algorithmic techniques that can provide personalized recommendations to their users.\n",
    "\n",
    "Amazon, for example, is well-known for its accurate selection of recommendations in its online site. Amazon's recommendation system is capable of intelligently analyzing and predicting customers' shopping preferences in order to offer them a list of recommended products. Amazon's recommendation algorithm is therefore a key element in using AI to improve the personalization of its website. For example, one of the baseline recommendation models that Amazon uses is item-to-item collaborative filtering, which scales to massive data sets and produces high-quality recommendations in real-time.\n",
    "\n",
    "----------------\n",
    "## **Objective:**\n",
    "----------------\n",
    "\n",
    "You are a Data Science Manager at Amazon, and have been given the task of building a recommendation system to recommend products to customers based on their previous ratings for other products. You have a collection of labeled data of Amazon reviews of products. The goal is to extract meaningful insights from the data and build a recommendation system that helps in recommending products to online consumers.\n",
    "\n",
    "-----------------------------\n",
    "## **Dataset:** \n",
    "-----------------------------\n",
    "\n",
    "The Amazon dataset contains the following attributes:\n",
    "\n",
    "- **userId:** Every user identified with a unique id\n",
    "- **productId:** Every product identified with a unique id\n",
    "- **Rating:** The rating of the corresponding product by the corresponding user\n",
    "- **timestamp:** Time of the rating. We **will not use this column** to solve the current problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmdPxJ2Q7W7p"
   },
   "source": [
    "**Note:** The code has some user defined functions that will be usefull while making recommendations and measure model performance, you can use these functions or can create your own functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoRfgjS2yekq"
   },
   "source": [
    "Sometimes, the installation of the surprise library, which is used to build recommendation systems, faces issues in Jupyter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ibk07-Cyekt"
   },
   "source": [
    "**Installing surprise library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "05HQoiZYlsbB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\julia\\onedrive\\documents\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\julia\\onedrive\\documents\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\julia\\onedrive\\documents\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.4)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (pyproject.toml): started\n",
      "  Building wheel for scikit-surprise (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-win_amd64.whl size=1297263 sha256=137e4dc5e63f802087b204db3bad81f6fcda812586316e588338ad0e73541924\n",
      "  Stored in directory: c:\\users\\julia\\appdata\\local\\pip\\cache\\wheels\\2a\\8f\\6e\\7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script surprise.exe is installed in 'c:\\Users\\Julia\\OneDrive\\Documents\\Anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fIt4jcFIm76"
   },
   "source": [
    "## **Importing the necessary libraries and overview of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "jzu2P-TT5JtP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import KNNBasic, SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrXYJAv95JtP"
   },
   "source": [
    "### **Loading the data**\n",
    "- Import the Dataset\n",
    "- Add column names ['user_id', 'prod_id', 'rating', 'timestamp']\n",
    "- Drop the column timestamp\n",
    "- Copy the data to another DataFrame called **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JGb-Hk1B5JtP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKM1MP6P0OYPR</td>\n",
       "      <td>0132793040</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2CX7LUOHB2NDG</td>\n",
       "      <td>0321732944</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2NWSAGRHCP8N5</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2WNBOD3WNDNKT</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1GI0U4ZRJA8WN</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id     prod_id  rating\n",
       "0   AKM1MP6P0OYPR  0132793040     5.0\n",
       "1  A2CX7LUOHB2NDG  0321732944     5.0\n",
       "2  A2NWSAGRHCP8N5  0439886341     1.0\n",
       "3  A2WNBOD3WNDNKT  0439886341     3.0\n",
       "4  A1GI0U4ZRJA8WN  0439886341     1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = 'ratings_Electronics.csv'  # Replace with the correct file path\n",
    "df_original = pd.read_csv(file_path, header=None)  # Assuming no header in the CSV\n",
    "\n",
    "# Add column names to the dataframe\n",
    "df_original.columns = ['user_id', 'prod_id', 'rating', 'timestamp']\n",
    "\n",
    "# Drop the 'timestamp' column\n",
    "df_cleaned = df_original.drop(columns=['timestamp'])\n",
    "\n",
    "# Copy the data to another DataFrame called 'df'\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "# Show the first few rows of the new DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVQnSG5g_9uX"
   },
   "source": [
    "**As this dataset is very large and has 7,824,482 observations, it is not computationally possible to build a model using this. Moreover, many users have only rated a few products and also some products are rated by very few users. Hence, we can reduce the dataset by considering certain logical assumptions.**\n",
    "\n",
    "Here, we will be taking users who have given at least 50 ratings, and the products that have at least 5 ratings, as when we shop online we prefer to have some number of ratings of a product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4yt9W7Q32EQQ"
   },
   "outputs": [],
   "source": [
    "# Get the column containing the users\n",
    "users = df.user_id\n",
    "\n",
    "# Create a dictionary from users to their number of ratings\n",
    "ratings_count = dict()\n",
    "\n",
    "for user in users:\n",
    "\n",
    "    # If we already have the user, just add 1 to their rating count\n",
    "    if user in ratings_count:        \n",
    "        ratings_count[user] += 1\n",
    "  \n",
    "    # Otherwise, set their rating count to 1\n",
    "    else:\n",
    "        ratings_count[user] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "19XB60dq2EQR"
   },
   "outputs": [],
   "source": [
    "# We want our users to have at least 50 ratings to be considered\n",
    "RATINGS_CUTOFF = 50\n",
    "\n",
    "remove_users = []\n",
    "\n",
    "for user, num_ratings in ratings_count.items():\n",
    "    if num_ratings < RATINGS_CUTOFF:\n",
    "        remove_users.append(user)\n",
    "\n",
    "df = df.loc[ ~ df.user_id.isin(remove_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "33UzK1D82EQS"
   },
   "outputs": [],
   "source": [
    "# Get the column containing the products\n",
    "prods = df.prod_id\n",
    "\n",
    "# Create a dictionary from products to their number of ratings\n",
    "ratings_count = dict()\n",
    "\n",
    "for prod in prods:\n",
    "    \n",
    "    # If we already have the product, just add 1 to its rating count\n",
    "    if prod in ratings_count:\n",
    "        ratings_count[prod] += 1\n",
    "    \n",
    "    # Otherwise, set their rating count to 1\n",
    "    else:\n",
    "        ratings_count[prod] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u6YE-lUp2EQT"
   },
   "outputs": [],
   "source": [
    "# We want our item to have at least 5 ratings to be considered\n",
    "RATINGS_CUTOFF = 5\n",
    "\n",
    "remove_users = []\n",
    "\n",
    "for user, num_ratings in ratings_count.items():\n",
    "    if num_ratings < RATINGS_CUTOFF:\n",
    "        remove_users.append(user)\n",
    "\n",
    "df_final = df.loc[~ df.prod_id.isin(remove_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aL1JZ00o5JtQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>A3LDPF5FMB782Z</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>A1A5KUIIIHFF4U</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>A2XIOXRRYX0KZY</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>AW3LX47IHPFRL</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>A1E3OB6QMBKRYZ</td>\n",
       "      <td>1400501466</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id     prod_id  rating\n",
       "1310  A3LDPF5FMB782Z  1400501466     5.0\n",
       "1322  A1A5KUIIIHFF4U  1400501466     1.0\n",
       "1335  A2XIOXRRYX0KZY  1400501466     3.0\n",
       "1451   AW3LX47IHPFRL  1400501466     5.0\n",
       "1456  A1E3OB6QMBKRYZ  1400501466     1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a few rows of the imported dataset\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuPoy_XfxhXZ"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0d0bWeG-sVB"
   },
   "source": [
    "### **Shape of the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyBVTRDTyek0"
   },
   "source": [
    "### **Check the number of rows and columns and provide observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ4eQKaY5JtQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65290, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and columns and provide observations\n",
    "# Get the shape of the dataset\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Slp-fgWQ-sVD"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "It still looks to be a large dataset, that's good, I thought it would be much smaller given all of the restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAMWm0nC-sVF"
   },
   "source": [
    "### **Data types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVrgMkye5JtQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id     object\n",
      "prod_id     object\n",
      "rating     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check Data types and provide observations\n",
    "# Check the data types of each column\n",
    "print(df_final.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4fOE02D-sVF"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "I have no observations, that all makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTMpOROT-sVG"
   },
   "source": [
    "### **Checking for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vt-VEjMA5JtQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id    0\n",
      "prod_id    0\n",
      "rating     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values present and provide observations\n",
    "# Check for missing values in each column\n",
    "missing_values = df_final.isnull().sum()\n",
    "\n",
    "# Display the missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMWuBNhI5JtR"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "There are no missing values? That's interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wETrCg48-sVG"
   },
   "source": [
    "### **Summary Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tYm30MXR5JtR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary of ratings are:\n",
      " count    65290.000000\n",
      "mean         4.294808\n",
      "std          0.988915\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max          5.000000\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "The summary of user ratings are:\n",
      " count    1540.000000\n",
      "mean       42.396104\n",
      "std        30.558505\n",
      "min         1.000000\n",
      "25%        23.000000\n",
      "50%        34.000000\n",
      "75%        52.000000\n",
      "max       295.000000\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get summary statistics of the 'rating' variable\n",
    "rating_stats = df_final['rating'].describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(\"The summary of ratings are:\\n\", rating_stats)\n",
    "\n",
    "# Number of ratings per user\n",
    "user_rating_counts = df_final['user_id'].value_counts()\n",
    "\n",
    "# Summary statistics for the number of ratings per user\n",
    "user_rating_stats = user_rating_counts.describe()\n",
    "\n",
    "# Show the summary stats\n",
    "print(\"\\nThe summary of user ratings are:\\n\", user_rating_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqW50EIJxhXc"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "The mean for ratings is very high, almost at maximum. Even the lowest 25% is 4/5 stars. Interesting. Popular products tend to have higher ratings, I suppose that makes sense!\n",
    "\n",
    "The mean for the number of reviews given by a person is 42 in this set. We did filter out those who didn't review many products, so this makes sense, too. People who write reviews tend to write a lot more than someone writing only one or two reviews. I'm unsure why someone still only has one review, that seems like an odd fluke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywyFrZIf5JtR"
   },
   "source": [
    "### **Checking the rating distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QbqhbEVe-sVH"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the bar plot and provide observations\n",
    "\n",
    "# Get the count of each rating\n",
    "rating_counts = df_final['rating'].value_counts().sort_index()\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "rating_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0jONrQv-sVH"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "This tends to be true when I go to Amazon - popular products have high ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HefpLdLJxhXd"
   },
   "source": [
    "### **Checking the number of unique users and items in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NbSom7195JtR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 65290\n",
      "Number of unique user IDs: 1540\n",
      "Number of unique product IDs: 5689\n"
     ]
    }
   ],
   "source": [
    "# Number of total rows in the data and number of unique user id and product id in the data\n",
    "# Get the total number of rows in the dataset\n",
    "total_rows = df_final.shape[0]\n",
    "\n",
    "# Get the number of unique user IDs\n",
    "unique_users = df_final['user_id'].nunique()\n",
    "\n",
    "# Get the number of unique product IDs\n",
    "unique_products = df_final['prod_id'].nunique()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Number of unique user IDs: {unique_users}\")\n",
    "print(f\"Number of unique product IDs: {unique_products}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qwgz6CUt-sVI"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "I expected there to be much more products, but the number is smaller than I expected. The number of users who post a lot of reviews is also much higher than I would've though, only about 1/3 of the number compared to the products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfDnhSS4-sVI"
   },
   "source": [
    "### **Users with the most number of ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "n7MX452q5JtR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 users based on the number of ratings:\n",
      "user_id\n",
      "ADLVFFE4VBT8      295\n",
      "A3OXHLG6DIBRW8    230\n",
      "A1ODOGXEYECQQ8    217\n",
      "A36K2N527TXXJN    212\n",
      "A25C2M3QF9G7OQ    203\n",
      "A680RUE1FDO8B     196\n",
      "A1UQBFCERIP7VJ    193\n",
      "A22CW0ZHY3NJH8    193\n",
      "AWPODHOB4GFWL     184\n",
      "AGVWTYW0ULXHT     179\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Top 10 users based on the number of ratings\n",
    "# Count the number of ratings per user\n",
    "user_rating_counts = df_final['user_id'].value_counts()\n",
    "\n",
    "# Get the top 10 users based on the number of ratings\n",
    "top_10_users = user_rating_counts.head(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 users based on the number of ratings:\")\n",
    "print(top_10_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X2w_jt9-sVI"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "This also makes sense, though I think I would've guessed the top one or two would have many more reviews than the others in the top 10. Otherwise, uneventful. They have a normal amount of reviews considering how large the dataset is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnYTx-Ol-sVg"
   },
   "source": [
    "**Now that we have explored and prepared the data, let's build the first recommendation system.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xYGrGVy5JtS"
   },
   "source": [
    "## **Model 1: Rank Based Recommendation System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yxZTj1UPxhXh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            average_rating  rating_count\n",
      "prod_id                                 \n",
      "B00LGQ6HL8             5.0             5\n",
      "B003DZJQQI             5.0            14\n",
      "B005FDXF2C             5.0             7\n",
      "B00I6CVPVC             5.0             7\n",
      "B00B9KOCYA             5.0             8\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average rating for each product \n",
    "avg_ratings = df_final.groupby('prod_id')['rating'].mean()\n",
    "\n",
    "# Calculate the count of ratings for each product\n",
    "rating_counts = df_final.groupby('prod_id')['rating'].count()\n",
    "\n",
    "# Create a dataframe with calculated average and count of ratings\n",
    "final_rating = pd.DataFrame({\n",
    "    'average_rating': avg_ratings,\n",
    "    'rating_count': rating_counts\n",
    "})\n",
    "\n",
    "# Sort the dataframe by average of ratings in the descending order\n",
    "final_rating_sorted = final_rating.sort_values(by='average_rating', ascending=False)\n",
    "\n",
    "# See the first five records of the \"final_rating\" dataset\n",
    "print(final_rating_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zKU__5s1xhXi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            average_rating  rating_count\n",
      "prod_id                                 \n",
      "B00L3YHF6O             5.0            14\n",
      "B00HG1L334             5.0            16\n",
      "B001TH7GUA             5.0            17\n",
      "B00C20KWP4             5.0            10\n",
      "B004Y1AYAC             5.0            11\n",
      "B00BLCVD9I             5.0            11\n",
      "B0058G40O8             5.0            12\n",
      "B00FZ9SMVU             5.0            10\n",
      "B005GI2VMG             5.0            10\n",
      "B0000B006W             5.0            12\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to get the top n products based on the highest average rating and minimum interactions\n",
    "def get_top_n_products(df, n=10, min_interactions=5):\n",
    "    # Calculate the average rating and count of ratings for each product\n",
    "    avg_ratings = df.groupby('prod_id')['rating'].mean()\n",
    "    rating_counts = df.groupby('prod_id')['rating'].count()\n",
    "\n",
    "    # Create a DataFrame with average ratings and count of ratings\n",
    "    products_stats = pd.DataFrame({\n",
    "        'average_rating': avg_ratings,\n",
    "        'rating_count': rating_counts\n",
    "    })\n",
    "\n",
    "    # Filter products that meet the minimum number of interactions\n",
    "    filtered_products = products_stats[products_stats['rating_count'] >= min_interactions]\n",
    "\n",
    "    # Sort the products by average rating in descending order\n",
    "    top_n_products = filtered_products.sort_values(by='average_rating', ascending=False).head(n)\n",
    "\n",
    "    return top_n_products\n",
    "\n",
    "# Example usage: Get the top 10 products with at least 10 interactions (ratings)\n",
    "top_10_products = get_top_n_products(df_final, n=10, min_interactions=10)\n",
    "\n",
    "# Display the top 10 products\n",
    "print(top_10_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8l6373PxhXi"
   },
   "source": [
    "### **Recommending top 5 products with 50 minimum interactions based on popularity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dBxdLiM_xhXi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9046\n",
      "MAE:  0.6590\n",
      "Root Mean Squared Error (RMSE): 0.9045643327992672\n",
      "Mean Absolute Error (MAE): 0.6590036573467458\n",
      "      prod_id  predicted_rating\n",
      "0  B00004ZCJE          4.877173\n",
      "1  B00007E7JU          5.000000\n",
      "2  B0002L5R78          5.000000\n",
      "3  B000BQ7GW8          5.000000\n",
      "4  B000HPV3RW          5.000000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the data for the model\n",
    "reader = Reader(rating_scale=(1, 5))  # Assuming ratings are between 1 and 5\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Create and train the SVD model (Singular Value Decomposition)\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 4: Make predictions for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Step 5: Evaluate the model using RMSE and other metrics\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Step 6: Calculate the number of ratings (interactions) per product\n",
    "rating_counts = df_final.groupby('prod_id').size()\n",
    "\n",
    "# Step 7: Filter products with at least the minimum number of interactions (50 ratings)\n",
    "popular_products = rating_counts[rating_counts >= 50].index\n",
    "\n",
    "# Step 8: Predict ratings for the entire dataset, including those with at least 50 ratings\n",
    "all_predictions = []\n",
    "for uid in df_final['user_id'].unique():\n",
    "    for pid in popular_products:\n",
    "        pred = model.predict(uid, pid)\n",
    "        all_predictions.append((uid, pid, pred.est))\n",
    "\n",
    "# Step 9: Convert the predictions into a DataFrame\n",
    "predicted_ratings = pd.DataFrame(all_predictions, columns=['user_id', 'prod_id', 'predicted_rating'])\n",
    "\n",
    "# Step 10: Sort by predicted rating in descending order and get top 5 products\n",
    "top_5_popular_products = predicted_ratings.groupby('prod_id').apply(lambda x: x.nlargest(1, 'predicted_rating')).reset_index(drop=True)\n",
    "\n",
    "# Display the top 5 products\n",
    "print(top_5_popular_products[['prod_id', 'predicted_rating']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9_xW_UMxhXj"
   },
   "source": [
    "### **Recommending top 5 products with 100 minimum interactions based on popularity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "dZgGZCUoxhXj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8933\n",
      "MAE:  0.6591\n",
      "      prod_id  predicted_rating\n",
      "0  B000N99BBC               5.0\n",
      "1  B002R5AM7C               5.0\n",
      "2  B002SZEOLG               5.0\n",
      "3  B002V88HFE               5.0\n",
      "4  B002WE6D44               5.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the data for the model\n",
    "reader = Reader(rating_scale=(1, 5))  # Assuming ratings are between 1 and 5\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Create and train the SVD model (Singular Value Decomposition)\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 4: Make predictions for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Step 5: Evaluate the model using RMSE and other metrics\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "\n",
    "# Step 6: Calculate the number of ratings (interactions) per product\n",
    "rating_counts = df_final.groupby('prod_id').size()\n",
    "\n",
    "# Step 7: Filter products with at least the minimum number of interactions (100 ratings)\n",
    "popular_products = rating_counts[rating_counts >= 100].index\n",
    "\n",
    "# Step 8: Predict ratings for the entire dataset, including those with at least 100 ratings\n",
    "all_predictions = []\n",
    "for uid in df_final['user_id'].unique():\n",
    "    for pid in popular_products:\n",
    "        pred = model.predict(uid, pid)\n",
    "        all_predictions.append((uid, pid, pred.est))\n",
    "\n",
    "# Step 9: Convert the predictions into a DataFrame\n",
    "predicted_ratings = pd.DataFrame(all_predictions, columns=['user_id', 'prod_id', 'predicted_rating'])\n",
    "\n",
    "# Step 10: Sort by predicted rating in descending order and get top 5 products\n",
    "top_5_popular_products = predicted_ratings.groupby('prod_id').apply(lambda x: x.nlargest(1, 'predicted_rating')).reset_index(drop=True)\n",
    "\n",
    "# Display the top 5 products\n",
    "print(top_5_popular_products[['prod_id', 'predicted_rating']].head(5))\n",
    "\n",
    "# I wish I knew what these products were. That would be super interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL-m68a15JtT",
    "outputId": "69132b0f-8d3f-4798-f6a0-249e17a3c822"
   },
   "source": [
    "We have recommended the **top 5** products by using the popularity recommendation system. Now, let's build a recommendation system using **collaborative filtering.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJI5kiiGvOOK"
   },
   "source": [
    "## **Model 2: Collaborative Filtering Recommendation System**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skzc0N1_nVNB"
   },
   "source": [
    "### **Building a baseline user-user similarity based recommendation system**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Uo_MYMnVNB"
   },
   "source": [
    "- Below, we are building **similarity-based recommendation systems** using `cosine` similarity and using **KNN to find similar users** which are the nearest neighbor to the given user.  \n",
    "- We will be using a new library, called `surprise`, to build the remaining models. Let's first import the necessary classes and functions from this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ1wEylUpexj"
   },
   "outputs": [],
   "source": [
    "# To compute the accuracy of models\n",
    "from surprise import accuracy\n",
    "\n",
    "# Class is used to parse a file containing ratings, data should be in structure - user ; item ; rating\n",
    "from surprise.reader import Reader\n",
    "\n",
    "# Class for loading datasets\n",
    "from surprise.dataset import Dataset\n",
    "\n",
    "# For tuning model hyperparameters\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# For splitting the rating data in train and test datasets\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# For implementing similarity-based recommendation system\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "# For implementing matrix factorization based recommendation system\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "\n",
    "# for implementing K-Fold cross-validation\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "# For implementing clustering-based recommendation system\n",
    "from surprise import CoClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54MqVAtDTsnl"
   },
   "source": [
    "**Before building the recommendation systems, let's  go over some basic terminologies we are going to use:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsxb3xhnTsnl"
   },
   "source": [
    "**Relevant item:** An item (product in this case) that is actually **rated higher than the threshold rating** is relevant, if the **actual rating is below the threshold then it is a non-relevant item**.  \n",
    "\n",
    "**Recommended item:** An item that's **predicted rating is higher than the threshold is a recommended item**, if the **predicted rating is below the threshold then that product will not be recommended to the user**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moyLUHCuTsnl"
   },
   "source": [
    "**False Negative (FN):** It is the **frequency of relevant items that are not recommended to the user**. If the relevant items are not recommended to the user, then the user might not buy the product/item. This would result in the **loss of opportunity for the service provider**, which they would like to minimize.\n",
    "\n",
    "**False Positive (FP):** It is the **frequency of recommended items that are actually not relevant**. In this case, the recommendation system is not doing a good job of finding and recommending the relevant items to the user. This would result in **loss of resources for the service provider**, which they would also like to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuvc2VaZTsnl"
   },
   "source": [
    "**Recall:** It is the **fraction of actually relevant items that are recommended to the user**, i.e., if out of 10 relevant products, 6 are recommended to the user then recall is 0.60. Higher the value of recall better is the model. It is one of the metrics to do the performance assessment of classification models.\n",
    "\n",
    "**Precision:** It is the **fraction of recommended items that are relevant actually**, i.e., if out of 10 recommended items, 6 are found relevant by the user then precision is 0.60. The higher the value of precision better is the model. It is one of the metrics to do the performance assessment of classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NLc36Y8Tsnm"
   },
   "source": [
    "**While making a recommendation system, it becomes customary to look at the performance of the model. In terms of how many recommendations are relevant and vice-versa, below are some most used performance metrics used in the assessment of recommendation systems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqF8fRBqTsnm"
   },
   "source": [
    "### **Precision@k, Recall@ k, and F1-score@k**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imMJNF0HTsnm"
   },
   "source": [
    "**Precision@k** - It is the **fraction of recommended items that are relevant in `top k` predictions**. The value of k is the number of recommendations to be provided to the user. One can choose a variable number of recommendations to be given to a unique user.  \n",
    "\n",
    "\n",
    "**Recall@k** - It is the **fraction of relevant items that are recommended to the user in `top k` predictions**.\n",
    "\n",
    "**F1-score@k** - It is the **harmonic mean of Precision@k and Recall@k**. When **precision@k and recall@k both seem to be important** then it is useful to use this metric because it is representative of both of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBW4BUhWTsnm"
   },
   "source": [
    "### **Some useful functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOBHKh0eTsnm"
   },
   "source": [
    "- Below function takes the **recommendation model** as input and gives the **precision@k, recall@k, and F1-score@k** for that model.  \n",
    "- To compute **precision and recall**, **top k** predictions are taken under consideration for each user.\n",
    "- We will use the precision and recall to compute the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Rxn-GahOTsnm"
   },
   "outputs": [],
   "source": [
    "def precision_recall_at_k(model, k = 10, threshold = 3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user\n",
    "    user_est_true = defaultdict(list)\n",
    "    \n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key = lambda x: x[0], reverse = True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. Therefore, we are setting Precision to 0 when n_rec_k is 0\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. Therefore, we are setting Recall to 0 when n_rel is 0\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "    \n",
    "    # Mean of all the predicted precisions are calculated.\n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "    \n",
    "    # Mean of all the predicted recalls are calculated.\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "    \n",
    "    accuracy.rmse(predictions)\n",
    "    \n",
    "    print('Precision: ', precision) # Command to print the overall precision\n",
    "    \n",
    "    print('Recall: ', recall) # Command to print the overall recall\n",
    "    \n",
    "    print('F_1 score: ', round((2*precision*recall)/(precision+recall), 3)) # Formula to compute the F-1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZmsamDVyek-"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "- To compute **precision and recall**, a **threshold of 3.5 and k value of 10 can be considered for the recommended and relevant ratings**.\n",
    "- Think about the performance metric to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hxjJMTwnVNB"
   },
   "source": [
    "Below we are loading the **`rating` dataset**, which is a **pandas DataFrame**, into a **different format called `surprise.dataset.DatasetAutoFolds`**, which is required by this library. To do this, we will be **using the classes `Reader` and `Dataset`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGfYDiOCpe4X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 52232\n",
      "Test set size: 13058\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiating Reader scale with expected rating scale (1-5 scale)\n",
    "# The Reader class will help us specify the rating scale (1-5) that is expected in the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# 2. Loading the rating dataset\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# 3. Splitting the data into train and test datasets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Verifying the split\n",
    "train_size = sum(1 for _ in trainset.all_ratings())  # Count the ratings in the training set\n",
    "test_size = len(testset)  # Directly count the number of ratings in the test set\n",
    "\n",
    "print(f\"Training set size: {train_size}\")  # Total number of ratings in the trainset\n",
    "print(f\"Test set size: {test_size}\")  # Total number of ratings in the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmHTEt7TnVNC"
   },
   "source": [
    "Now, we are **ready to build the first baseline similarity-based recommendation system** using the cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVDfVHB4tQfU"
   },
   "source": [
    "### **Building the user-user Similarity-based Recommendation System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vO3FL7iape8A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9025\n",
      "Precision@10: 0.844\n",
      "Recall@10: 0.877\n",
      "F1-score@10: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Declaring similarity options for user-user similarity\n",
    "# This got complicated because the RSME value was printing five times, I fixed it to only print once\n",
    "\n",
    "# Step 2: Initialize the SVD model (for collaborative filtering)\n",
    "model = SVD()\n",
    "\n",
    "# Step 3: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # The rating scale is from 1 to 5\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 4: Splitting the data into train and test datasets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 5: Fit the SVD model on the training data\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Step 7: Compute RMSE for SVD model (print only once)\n",
    "rmse_value = accuracy.rmse(predictions)\n",
    "\n",
    "def precision_recall_at_k(model, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "    \n",
    "    # First map the predictions to each user\n",
    "    user_est_true = defaultdict(list)\n",
    "    \n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    # The rest of the function calculates precision and recall. Ensure RMSE is NOT calculated here.\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "    \n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1_score_at_k = round((2 * precision * recall) / (precision + recall), 3)\n",
    "    else:\n",
    "        f1_score_at_k = 0\n",
    "    \n",
    "    # No RMSE calculation here, it should only be printed once outside this function.\n",
    "    \n",
    "    print(f\"Precision@{k}: {precision}\")\n",
    "    print(f\"Recall@{k}: {recall}\")\n",
    "    print(f\"F1-score@{k}: {f1_score_at_k}\")\n",
    "\n",
    "# Step 9: Compute precision@k, recall@k, and f_1 score using the precision_recall_at_k function\n",
    "precision_recall_at_k(model, k=10, threshold=3.5)  # Here, k=10 and threshold=3.5 are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEuJK_A9Tsnn"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "This is surprisingly hard, I had a very difficult time reducing the RMSE. It's still not great, but I don't want to overcomplicate this given the other values aren't too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reFD0-nsnVNC"
   },
   "source": [
    "Let's now **predict rating for a user with `userId=A3LDPF5FMB782Z` and `productId=1400501466`** as shown below. Here the user has already interacted or watched the product with productId '1400501466' and given a rating of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Sxd23bZ9pe_x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual rating for User A3LDPF5FMB782Z and Product 1400501466: 5.0\n",
      "Predicted rating for User A3LDPF5FMB782Z and Product 1400501466: 4.294378924797059\n"
     ]
    }
   ],
   "source": [
    "# Predicting the rating for a specific user-product pair\n",
    "user_id = 'A3LDPF5FMB782Z'  # Example userId\n",
    "product_id = '1400501466'    # Example productId\n",
    "\n",
    "# Step 1: Predict the rating for this user-product pair\n",
    "prediction = model.predict(user_id, product_id)\n",
    "\n",
    "# Step 2: Retrieve the actual rating from the dataset (if the user has already interacted with the product)\n",
    "if len(df_final[(df_final['user_id'] == user_id) & (df_final['prod_id'] == product_id)]) > 0:\n",
    "    actual_rating = df_final[(df_final['user_id'] == user_id) & (df_final['prod_id'] == product_id)]['rating'].values[0]\n",
    "    print(f\"Actual rating for User {user_id} and Product {product_id}: {actual_rating}\")\n",
    "else:\n",
    "    actual_rating = None\n",
    "    print(f\"User {user_id} has not interacted with Product {product_id} yet.\")\n",
    "\n",
    "# Step 3: Print the predicted rating\n",
    "print(f\"Predicted rating for User {user_id} and Product {product_id}: {prediction.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJcqG_wemRH"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "That's not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj6ecbglTsno"
   },
   "source": [
    "Below is the function to find the **list of users who have not seen the product with product id \"1400501466\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_users_not_interacted_with(n, data, prod_id):\n",
    "    users_interacted_with_product = set(data[data['prod_id'] == prod_id]['user_id'])\n",
    "    all_users = set(data['user_id'])\n",
    "    return list(all_users.difference(users_interacted_with_product))[:n] # where n is the number of elements to get in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCRBMD-RTsno"
   },
   "outputs": [],
   "source": [
    "# Find unique user_id where prod_id is not equal to \"1400501466\"\n",
    "n_users_not_interacted_with(5, df_final, '1400501466')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT42ecaSTsno"
   },
   "source": [
    "* It can be observed from the above list that **user \"A2UOHALGF2X77Q\" has not seen the product with productId \"1400501466\"** as this user id is a part of the above list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXSgq8OEnVNE"
   },
   "source": [
    "**Below we are predicting rating for `userId=A2UOHALGF2X77Q` and `prod_id=1400501466`.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "PbFcBj1PpfEV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for User A2UOHALGF2X77Q and Product 1400501466: 4.178688122176693\n"
     ]
    }
   ],
   "source": [
    "# Predict the rating for user 'A2UOHALGF2X77Q' and product '1400501466'\n",
    "user_id = 'A2UOHALGF2X77Q'  # User who hasn't seen the product\n",
    "product_id = '1400501466'   # Product the user hasn't interacted with\n",
    "\n",
    "# Predicting the rating for this user-product pair\n",
    "prediction = model.predict(user_id, product_id)\n",
    "\n",
    "# Print the predicted rating\n",
    "print(f\"Predicted rating for User {user_id} and Product {product_id}: {prediction.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02rwld8yemRI"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "I wish I could see their item history and what this item is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejjof6csnVNF"
   },
   "source": [
    "### **Improving similarity-based recommendation system by tuning its hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2j4VvfQnVNF"
   },
   "source": [
    "Below, we will be tuning hyperparameters for the `KNNBasic` algorithm. Let's try to understand some of the hyperparameters of the KNNBasic algorithm:\n",
    "\n",
    "- **k** (int) – The (max) number of neighbors to take into account for aggregation. Default is 40.\n",
    "- **min_k** (int) – The minimum number of neighbors to take into account for aggregation. If there are not enough neighbors, the prediction is set to the global mean of all ratings. Default is 1.\n",
    "- **sim_options** (dict) – A dictionary of options for the similarity measure. And there are four similarity measures available in surprise - \n",
    "    - cosine\n",
    "    - msd (default)\n",
    "    - Pearson\n",
    "    - Pearson baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "9LmPbSUSTsnp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE for Cosine similarity: 1.0692089283361854\n",
      "Best parameters for Cosine similarity: {'k': 40, 'min_k': 1, 'sim_options': {'name': 'cosine', 'user_based': False}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Reduce the parameter grid size\n",
    "param_grid_cosine = {\n",
    "    'k': [30, 40],  # Number of neighbors to consider\n",
    "    'min_k': [1],  # Minimum number of neighbors to consider\n",
    "    'sim_options': {\n",
    "        'name': ['cosine'],  # Cosine similarity (item-item)\n",
    "        'user_based': [False]  # False for item-item, True for user-user\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 3: Perform 2-fold cross-validation with parallelism\n",
    "grid_search_cosine = GridSearchCV(KNNBasic, param_grid_cosine, measures=['RMSE'], cv=2, n_jobs=-1)\n",
    "grid_search_cosine.fit(data)\n",
    "\n",
    "# Best RMSE for Cosine similarity\n",
    "print(f\"Best RMSE for Cosine similarity: {grid_search_cosine.best_score['rmse']}\")\n",
    "print(f\"Best parameters for Cosine similarity: {grid_search_cosine.best_params['rmse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2fHNvu7nVNF"
   },
   "source": [
    "Once the grid search is **complete**, we can get the **optimal values for each of those hyperparameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHWgxu_YnVNG"
   },
   "source": [
    "Now, let's build the **final model by using tuned values of the hyperparameters**, which we received by using **grid search cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "PujRJA8X_JEJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Precision@10: 0.836\n",
      "Recall@10: 0.852\n",
      "F1-Score@10: 0.844\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the best hyperparameters from GridSearchCV\n",
    "best_params = grid_search_cosine.best_params['rmse']\n",
    "best_k = best_params['k']\n",
    "best_min_k = best_params['min_k']\n",
    "best_sim_options = best_params['sim_options']\n",
    "\n",
    "# Step 2: Create an instance of KNNBasic with the best hyperparameters\n",
    "model = KNNBasic(k=best_k, min_k=best_min_k, sim_options=best_sim_options)\n",
    "\n",
    "# Step 3: Load the dataset and split it into training and test sets\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 4: Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 5: Compute Precision@k and Recall@k for the trained model\n",
    "def precision_recall_at_k(model, testset, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "    \n",
    "    user_est_true = defaultdict(list)\n",
    "    \n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    # Mean of all the predicted precisions are calculated\n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "\n",
    "    # Mean of all the predicted recalls are calculated\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "\n",
    "    # F1-Score\n",
    "    f1 = round((2 * precision * recall) / (precision + recall), 3)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Precision@{k}: {precision}\")\n",
    "    print(f\"Recall@{k}: {recall}\")\n",
    "    print(f\"F1-Score@{k}: {f1}\")\n",
    "    \n",
    "# Step 6: Compute precision@k, recall@k for k=10\n",
    "precision_recall_at_k(model, testset, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHsWvFjKTsnp"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "This is a solid start for a model, I do wish it were better but it's been surprisingly hard to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhcAXK0CnVNG"
   },
   "source": [
    "### **Steps:**\n",
    "- **Predict rating for the user with `userId=\"A3LDPF5FMB782Z\"`, and `prod_id= \"1400501466\"` using the optimized model**\n",
    "- **Predict rating for `userId=\"A2UOHALGF2X77Q\"` who has not interacted with `prod_id =\"1400501466\"`, by using the optimized model**\n",
    "- **Compare the output with the output from the baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "FgV63lHiq1TV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Actual rating for user A3LDPF5FMB782Z and product 1400501466: 5.0\n",
      "Predicted rating for user A3LDPF5FMB782Z and product 1400501466: 4.294378924797059\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the user and product for prediction\n",
    "user_id_1 = 'A3LDPF5FMB782Z'\n",
    "prod_id_1 = '1400501466'\n",
    "\n",
    "# Step 2: Create an instance of KNNBasic with the best hyperparameters (from grid search)\n",
    "model = KNNBasic(k=40, min_k=1, sim_options={'name': 'cosine', 'user_based': False})\n",
    "\n",
    "# Step 3: Load the dataset and split it into training and test sets\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 4: Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 5: Make a prediction for the specified user and product\n",
    "prediction_1 = model.predict(user_id_1, prod_id_1)\n",
    "\n",
    "# Step 6: Retrieve the actual rating from the dataset (if the user has already interacted with the product)\n",
    "if len(df_final[(df_final['user_id'] == user_id_1) & (df_final['prod_id'] == prod_id_1)]) > 0:\n",
    "    actual_rating = df_final[(df_final['user_id'] == user_id_1) & (df_final['prod_id'] == prod_id_1)]['rating'].values[0]\n",
    "    print(f\"Actual rating for user {user_id_1} and product {prod_id_1}: {actual_rating}\")\n",
    "else:\n",
    "    actual_rating = None\n",
    "    print(f\"User {user_id_1} has not interacted with product {prod_id_1} yet.\")\n",
    "\n",
    "# Step 7: Print the predicted rating\n",
    "print(f\"Predicted rating for user {user_id_1} and product {prod_id_1}: {prediction_1.est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "HXO2Ztjhq1bN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Predicted rating for user A2UOHALGF2X77Q and product 1400501466: 4.5\n"
     ]
    }
   ],
   "source": [
    "# Use sim_user_user_optimized model to recommend for userId \"A2UOHALGF2X77Q\" and productId \"1400501466\"\n",
    "# Step 1: Define the user and product for prediction\n",
    "user_id_2 = 'A2UOHALGF2X77Q'\n",
    "prod_id_2 = '1400501466'\n",
    "\n",
    "# Step 2: Create an instance of KNNBasic with the best hyperparameters (from grid search)\n",
    "model = KNNBasic(k=40, min_k=1, sim_options={'name': 'cosine', 'user_based': False})\n",
    "\n",
    "# Step 3: Load the dataset and split it into training and test sets\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 4: Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 5: Make a prediction for the user that has not interacted with the product\n",
    "prediction_2 = model.predict(user_id_2, prod_id_2)\n",
    "\n",
    "# Step 6: Print the predicted rating\n",
    "print(f\"Predicted rating for user {user_id_2} and product {prod_id_2}: {prediction_2.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5i-OPprNF2e"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "These always seem like the people would like the product. I still don't know how to evaluate the second one.\n",
    "The first guess isn't too bad, but I would assume they gave rated the item a 4, not 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op_zwO_FnVNH"
   },
   "source": [
    "### **Identifying similar users to a given user (nearest neighbors)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2QsfqhanVNH"
   },
   "source": [
    "We can also find out **similar users to a given user** or its **nearest neighbors** based on this KNNBasic algorithm. Below, we are finding the 5 most similar users to the first user in the list with internal id 0, based on the `msd` distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "TbFle7cKmBJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "The 5 most similar users to the first user (internal id 0):\n",
      "User ID: A3EZEP0FX5BC1P\n",
      "User ID: A3QNQQKJTL76H0\n",
      "User ID: AEL6CQNQXONBX\n",
      "User ID: A3RGJ1FXOB1ZLL\n",
      "User ID: A11T807LX2EF00\n"
     ]
    }
   ],
   "source": [
    "# 0 is the inner id of the above user\n",
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Train the KNNBasic model with 'msd' similarity measure (Mean Squared Difference)\n",
    "sim_options = {\n",
    "    'name': 'msd',  # Use the msd (Mean Squared Difference) similarity measure\n",
    "    'user_based': True  # User-based similarity (user-user)\n",
    "}\n",
    "\n",
    "model_msd = KNNBasic(sim_options=sim_options)\n",
    "model_msd.fit(trainset)\n",
    "\n",
    "# Step 4: Find the 5 most similar users to the first user (user with internal id 0)\n",
    "# The internal id of the first user is 0, as specified\n",
    "user_inner_id = 0\n",
    "neighbors = model_msd.get_neighbors(user_inner_id, k=5)\n",
    "\n",
    "# Step 5: Output the user ids of the 5 most similar users\n",
    "# `get_neighbors` returns the internal ids, so we'll convert them back to user ids\n",
    "print(f\"The 5 most similar users to the first user (internal id {user_inner_id}):\")\n",
    "for neighbor in neighbors:\n",
    "    # Converting internal id back to user_id\n",
    "    user_id = trainset.to_raw_uid(neighbor)\n",
    "    print(f\"User ID: {user_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0NsrX_anVNH"
   },
   "source": [
    "### **Implementing the recommendation algorithm based on optimized KNNBasic model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3ESobDynVNI"
   },
   "source": [
    "Below we will be implementing a function where the input parameters are:\n",
    "\n",
    "- data: A **rating** dataset\n",
    "- user_id: A user id **against which we want the recommendations**\n",
    "- top_n: The **number of products we want to recommend**\n",
    "- algo: the algorithm we want to use **for predicting the ratings**\n",
    "- The output of the function is a **set of top_n items** recommended for the given user_id based on the given algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "vW9V1Tk65HlY"
   },
   "outputs": [],
   "source": [
    "def get_recommendations(data, user_id, top_n, algo):\n",
    "    \n",
    "    # Creating an empty list to store the recommended product ids\n",
    "    recommendations = []\n",
    "    \n",
    "    # Creating an user item interactions matrix \n",
    "    user_item_interactions_matrix = data.pivot(index = 'user_id', columns = 'prod_id', values = 'rating')\n",
    "    \n",
    "    # Extracting those product ids which the user_id has not interacted yet\n",
    "    non_interacted_products = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n",
    "    \n",
    "    # Looping through each of the product ids which user_id has not interacted yet\n",
    "    for item_id in non_interacted_products:\n",
    "        \n",
    "        # Predicting the ratings for those non interacted product ids by this user\n",
    "        est = algo.predict(user_id, item_id).est\n",
    "        \n",
    "        # Appending the predicted ratings\n",
    "        recommendations.append((item_id, est))\n",
    "\n",
    "    # Sorting the predicted ratings in descending order\n",
    "    recommendations.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return recommendations[:top_n] # Returing top n highest predicted rating products for this user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj_S7kh4nVNI"
   },
   "source": [
    "**Predicting top 5 products for userId = \"A3LDPF5FMB782Z\" with similarity based recommendation system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "qWbR85mI5Hrk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Top 5 product recommendations for user A3LDPF5FMB782Z:\n",
      "Product ID: B00001P4XH, Predicted Rating: 5\n",
      "Product ID: B00001WRSJ, Predicted Rating: 5\n",
      "Product ID: B000021YU8, Predicted Rating: 5\n",
      "Product ID: B000026D8E, Predicted Rating: 5\n",
      "Product ID: B000031KIM, Predicted Rating: 5\n"
     ]
    }
   ],
   "source": [
    "# Making top 5 recommendations for user_id \"A3LDPF5FMB782Z\" with a similarity-based recommendation engine\n",
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Train the KNNBasic model (e.g., using Cosine similarity)\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity (item-item)\n",
    "    'user_based': False  # Item-based collaborative filtering\n",
    "}\n",
    "\n",
    "model = KNNBasic(sim_options=sim_options)\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 4: Define the function to get top n product recommendations\n",
    "def get_recommendations(data, user_id, top_n, algo):\n",
    "    # Creating an empty list to store the recommended product ids\n",
    "    recommendations = []\n",
    "    \n",
    "    # Creating a user-item interaction matrix \n",
    "    user_item_interactions_matrix = data.pivot(index='user_id', columns='prod_id', values='rating')\n",
    "    \n",
    "    # Extracting those product ids which the user_id has not interacted yet\n",
    "    non_interacted_products = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n",
    "    \n",
    "    # Looping through each of the product ids which user_id has not interacted yet\n",
    "    for item_id in non_interacted_products:\n",
    "        # Predicting the ratings for those non-interacted product ids by this user\n",
    "        est = algo.predict(user_id, item_id).est\n",
    "        \n",
    "        # Appending the predicted ratings\n",
    "        recommendations.append((item_id, est))\n",
    "\n",
    "    # Sorting the predicted ratings in descending order\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return recommendations[:top_n]  # Returning top n highest predicted rating products for this user\n",
    "\n",
    "# Step 5: Get top 5 product recommendations for userId = \"A3LDPF5FMB782Z\"\n",
    "user_id = \"A3LDPF5FMB782Z\"\n",
    "top_n = 5\n",
    "top_recommendations = get_recommendations(df_final, user_id, top_n, model)\n",
    "\n",
    "# Step 6: Print the recommendations\n",
    "print(f\"Top {top_n} product recommendations for user {user_id}:\")\n",
    "for product_id, predicted_rating in top_recommendations:\n",
    "    print(f\"Product ID: {product_id}, Predicted Rating: {predicted_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "b5WfIX0Z6_q2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_id</th>\n",
       "      <th>predicted_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00001P4XH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000021YU8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00002EQCW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00003G1RG</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prod_id  predicted_ratings\n",
       "0  B00001P4XH                  5\n",
       "1  B00001P4ZH                  5\n",
       "2  B000021YU8                  5\n",
       "3  B00002EQCW                  5\n",
       "4  B00003G1RG                  5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Train the KNNBasic model with 'cosine' similarity measure (item-item)\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity (item-item)\n",
    "    'user_based': False  # Item-based collaborative filtering\n",
    "}\n",
    "\n",
    "model = KNNBasic(sim_options=sim_options)\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 4: Define the function to get top n product recommendations and build a DataFrame\n",
    "def get_recommendations(data, user_id, top_n, algo):\n",
    "    # Creating an empty list to store the recommended product ids and predicted ratings\n",
    "    recommendations = []\n",
    "    \n",
    "    # Creating a user-item interaction matrix\n",
    "    user_item_interactions_matrix = data.pivot(index='user_id', columns='prod_id', values='rating')\n",
    "    \n",
    "    # Extracting those product ids which the user_id has not interacted yet\n",
    "    non_interacted_products = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n",
    "    \n",
    "    # Looping through each of the product ids the user has not interacted with yet\n",
    "    for item_id in non_interacted_products:\n",
    "        # Predicting the ratings for those non-interacted product ids by this user\n",
    "        est = algo.predict(user_id, item_id).est\n",
    "        \n",
    "        # Appending the predicted ratings and product ids\n",
    "        recommendations.append((item_id, est))\n",
    "\n",
    "    # Sorting the predicted ratings in descending order\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Returning top n highest predicted rating products for this user\n",
    "    return recommendations[:top_n]\n",
    "\n",
    "# Step 5: Get top 5 product recommendations for userId = \"A3LDPF5FMB782Z\"\n",
    "user_id = \"A3LDPF5FMB782Z\"\n",
    "top_n = 5\n",
    "top_recommendations = get_recommendations(df_final, user_id, top_n, model)\n",
    "\n",
    "# Step 6: Build a DataFrame from the recommendations list\n",
    "df_recommendations = pd.DataFrame(top_recommendations, columns=[\"prod_id\", \"predicted_ratings\"])\n",
    "\n",
    "# Step 7: Display the DataFrame in the notebook\n",
    "df_recommendations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgbzJKk7Tsnr"
   },
   "source": [
    "### **Item-Item Similarity-based Collaborative Filtering Recommendation System**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTJu_2hcTsnr"
   },
   "source": [
    "* Above we have seen **similarity-based collaborative filtering** where similarity is calculated **between users**. Now let us look into similarity-based collaborative filtering where similarity is seen **between items**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "W5RMcdzjTsns",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Precision@10: 0.828\n",
      "Recall@10: 0.843\n",
      "F1-Score@10: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Declare the similarity options for item-item similarity (using cosine similarity)\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Cosine similarity measure\n",
    "    'user_based': False  # Item-based collaborative filtering (False for item-item)\n",
    "}\n",
    "\n",
    "# Step 4: Initialize the KNNBasic algorithm with item-item similarity\n",
    "model_item_item = KNNBasic(sim_options=sim_options, random_state=1)\n",
    "\n",
    "# Step 5: Train the algorithm on the training data\n",
    "model_item_item.fit(trainset)\n",
    "\n",
    "# Step 6: Predict ratings for the test set\n",
    "predictions = model_item_item.test(testset)\n",
    "\n",
    "# Step 7: Compute precision@k, recall@k, and F1-score with k=10\n",
    "def precision_recall_at_k(model, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    \n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    # Mean of all the predicted precisions are calculated\n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "\n",
    "    # Mean of all the predicted recalls are calculated\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "\n",
    "    # F1-Score\n",
    "    f1 = round((2 * precision * recall) / (precision + recall), 3)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Precision@{k}: {precision}\")\n",
    "    print(f\"Recall@{k}: {recall}\")\n",
    "    print(f\"F1-Score@{k}: {f1}\")\n",
    "\n",
    "# Step 8: Compute precision@k, recall@k, and F1-score with k=10\n",
    "precision_recall_at_k(model_item_item, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni9LoeUVTsns"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "Again, I do wish this were better. I need to learn more advanced modeling techniques to get to ideal values. But for now, this is isn't bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFbcDQmxTsns"
   },
   "source": [
    "Let's now **predict a rating for a user with `userId = A3LDPF5FMB782Z` and `prod_Id = 1400501466`** as shown below. Here the user has already interacted or watched the product with productId \"1400501466\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "JsF-aaWYTsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Actual rating for user A3LDPF5FMB782Z and product 1400501466: 5.0\n",
      "Predicted rating for user A3LDPF5FMB782Z and product 1400501466: 4.2272727272727275\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Declare the similarity options for item-item similarity (using cosine similarity)\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Cosine similarity measure\n",
    "    'user_based': False  # Item-based collaborative filtering (False for item-item)\n",
    "}\n",
    "\n",
    "# Step 4: Initialize the KNNBasic algorithm with item-item similarity\n",
    "model_item_item = KNNBasic(sim_options=sim_options, random_state=1)\n",
    "\n",
    "# Step 5: Train the algorithm on the training data\n",
    "model_item_item.fit(trainset)\n",
    "\n",
    "# Step 6: Predict the rating for the sample user (A3LDPF5FMB782Z) and product (1400501466)\n",
    "user_id = \"A3LDPF5FMB782Z\"\n",
    "prod_id = \"1400501466\"\n",
    "\n",
    "# Predict the rating using the model\n",
    "prediction = model_item_item.predict(user_id, prod_id)\n",
    "\n",
    "# Step 7: Retrieve the actual rating from the dataset\n",
    "actual_rating = df_final[(df_final['user_id'] == user_id) & (df_final['prod_id'] == prod_id)]['rating'].values[0]\n",
    "\n",
    "# Step 8: Print both actual and predicted ratings\n",
    "print(f\"Actual rating for user {user_id} and product {prod_id}: {actual_rating}\")\n",
    "print(f\"Predicted rating for user {user_id} and product {prod_id}: {prediction.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "That's not too bad, but if I were looking at just the prediction, I would likely assume they gave it a 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqKGZoAtTsns"
   },
   "source": [
    "Below we are **predicting rating for the `userId = A2UOHALGF2X77Q` and `prod_id = 1400501466`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "5yILOxXRTsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Predicted rating for user A2UOHALGF2X77Q and product 1400501466: 4.5\n"
     ]
    }
   ],
   "source": [
    "# Predicting rating for a sample user with a non interacted product\n",
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Declare the similarity options for item-item similarity (using cosine similarity)\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Cosine similarity measure\n",
    "    'user_based': False  # Item-based collaborative filtering (False for item-item)\n",
    "}\n",
    "\n",
    "# Step 4: Initialize the KNNBasic algorithm with item-item similarity\n",
    "model_item_item = KNNBasic(sim_options=sim_options, random_state=1)\n",
    "\n",
    "# Step 5: Train the algorithm on the training data\n",
    "model_item_item.fit(trainset)\n",
    "\n",
    "# Step 6: Predict the rating for the sample user (A2UOHALGF2X77Q) and product (1400501466)\n",
    "user_id = \"A2UOHALGF2X77Q\"\n",
    "prod_id = \"1400501466\"\n",
    "\n",
    "prediction = model_item_item.predict(user_id, prod_id)\n",
    "\n",
    "# Step 7: Print the predicted rating\n",
    "print(f\"Predicted rating for user {user_id} and product {prod_id}: {prediction.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meSvpNLj_EjD"
   },
   "source": [
    "### **Hyperparameter tuning the item-item similarity-based model**\n",
    "- Use the following values for the param_grid and tune the model.\n",
    "  - 'k': [10, 20, 30]\n",
    "  - 'min_k': [3, 6, 9]\n",
    "  - 'sim_options': {'name': ['msd', 'cosine']\n",
    "  - 'user_based': [False]\n",
    "- Use GridSearchCV() to tune the model using the 'rmse' measure\n",
    "- Print the best score and best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "f5bcZ3HgTsnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 0.9937875759255935\n",
      "Best parameters for RMSE: {'k': 20, 'min_k': 3, 'sim_options': {'name': 'cosine', 'user_based': False}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Set up a smaller parameter grid to tune the hyperparameters (smaller k and min_k ranges)\n",
    "param_grid = {\n",
    "    'k': [20],  # Smaller range for the number of neighbors to consider\n",
    "    'min_k': [3],  # Smaller range for the minimum number of neighbors\n",
    "    'sim_options': {\n",
    "        'name': ['cosine'],  # Only cosine similarity (for speed)\n",
    "        'user_based': [False]  # Item-based collaborative filtering\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 4: Perform 2-fold cross-validation with parallelism (n_jobs=-1 for speed)\n",
    "grid_search = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=2, n_jobs=-1)\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Step 5: Find the best RMSE score\n",
    "print(f\"Best RMSE score: {grid_search.best_score['rmse']}\")\n",
    "\n",
    "# Step 6: Find the combination of parameters that gave the best RMSE score\n",
    "print(f\"Best parameters for RMSE: {grid_search.best_params['rmse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1psOlx6zTsnt"
   },
   "source": [
    "Once the **grid search** is complete, we can get the **optimal values for each of those hyperparameters as shown above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrSTaQemTsnt"
   },
   "source": [
    "Now let's build the **final model** by using **tuned values of the hyperparameters** which we received by using grid search cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOS9Dwnd_LN6"
   },
   "source": [
    "### **Use the best parameters from GridSearchCV to build the optimized item-item similarity-based model. Compare the performance of the optimized model with the baseline model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "dSeiM1qeTsnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9907\n",
      "Precision@10: 0.833\n",
      "Recall@10: 0.874\n",
      "F1-Score@10: 0.853\n",
      "RMSE: 0.9907449714845942\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Use the best parameters from GridSearchCV\n",
    "# You would extract these from the results of GridSearchCV\n",
    "best_k = 20  # Example best k found from GridSearchCV\n",
    "best_min_k = 3  # Example best min_k found from GridSearchCV\n",
    "best_similarity = 'cosine'  # Example best similarity measure found from GridSearchCV\n",
    "\n",
    "# Step 4: Create the optimized KNNBasic model with best hyperparameters\n",
    "sim_options = {\n",
    "    'name': best_similarity,  # Similarity measure (e.g., cosine)\n",
    "    'user_based': False  # Item-based collaborative filtering\n",
    "}\n",
    "optimized_model = KNNBasic(k=best_k, min_k=best_min_k, sim_options=sim_options)\n",
    "\n",
    "# Step 5: Train the model on the training data\n",
    "optimized_model.fit(trainset)\n",
    "\n",
    "# Step 6: Compute precision@k, recall@k, F1-score and RMSE for the optimized model\n",
    "def precision_recall_at_k(model, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    user_est_true = defaultdict(list)\n",
    "    \n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    # Mean of all the predicted precisions are calculated\n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "\n",
    "    # Mean of all the predicted recalls are calculated\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "\n",
    "    # F1-Score\n",
    "    f1 = round((2 * precision * recall) / (precision + recall), 3)\n",
    "\n",
    "    # RMSE score\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Precision@{k}: {precision}\")\n",
    "    print(f\"Recall@{k}: {recall}\")\n",
    "    print(f\"F1-Score@{k}: {f1}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Step 7: Compute precision@k, recall@k, F1-score and RMSE for the optimized model\n",
    "precision_recall_at_k(optimized_model, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCXKnMI8Tsnt"
   },
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "This is similar to other models I already did, I tried to improve the earlier models so they ended up having similar values. Almost identical, actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbcj_H94Tsnt"
   },
   "source": [
    "### **Steps:**\n",
    "- **Predict rating for the user with `userId=\"A3LDPF5FMB782Z\"`, and `prod_id= \"1400501466\"` using the optimized model**\n",
    "- **Predict rating for `userId=\"A2UOHALGF2X77Q\"` who has not interacted with `prod_id =\"1400501466\"`, by using the optimized model**\n",
    "- **Compare the output with the output from the baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "gIBRRvdoTsnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Actual rating for User A3LDPF5FMB782Z and Product 1400501466: 5.0\n",
      "Predicted rating for User A3LDPF5FMB782Z and Product 1400501466: 4.384615384615385\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the user and product for prediction\n",
    "user_id_1 = 'A3LDPF5FMB782Z'  # Example userId\n",
    "prod_id_1 = '1400501466'      # Example productId\n",
    "\n",
    "# Step 2: Create an instance of KNNBasic with the best hyperparameters (from grid search)\n",
    "model = KNNBasic(k=40, min_k=1, sim_options={'name': 'cosine', 'user_based': False})\n",
    "\n",
    "# Step 3: Load the dataset and split it into training and test sets\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 4: Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 5: Make a prediction for the specified user and product\n",
    "prediction_1 = model.predict(user_id_1, prod_id_1)\n",
    "\n",
    "# Step 6: Check if the user has already interacted with the product\n",
    "if len(df_final[(df_final['user_id'] == user_id_1) & (df_final['prod_id'] == prod_id_1)]) > 0:\n",
    "    actual_rating = df_final[(df_final['user_id'] == user_id_1) & (df_final['prod_id'] == prod_id_1)]['rating'].values[0]\n",
    "    print(f\"Actual rating for User {user_id_1} and Product {prod_id_1}: {actual_rating}\")\n",
    "else:\n",
    "    actual_rating = None\n",
    "    print(f\"User {user_id_1} has not interacted with Product {prod_id_1} yet.\")\n",
    "\n",
    "# Step 7: Print the predicted rating\n",
    "print(f\"Predicted rating for User {user_id_1} and Product {prod_id_1}: {prediction_1.est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User A2UOHALGF2X77Q has not interacted with Product 1400501466 yet.\n",
      "Predicted rating for User A2UOHALGF2X77Q and Product 1400501466: 4.75\n"
     ]
    }
   ],
   "source": [
    "# Predicting the rating for user \"A2UOHALGF2X77Q\" and product \"1400501466\" using the optimized model\n",
    "\n",
    "user_id_2 = 'A2UOHALGF2X77Q'  # Example userId\n",
    "prod_id_2 = '1400501466'      # Example productId\n",
    "\n",
    "# Make prediction with the optimized item-item similarity model\n",
    "prediction_optimized_2 = optimized_model.predict(user_id_2, prod_id_2)\n",
    "\n",
    "# Step 6: Check if the user has already interacted with the product\n",
    "if len(df_final[(df_final['user_id'] == user_id_2) & (df_final['prod_id'] == prod_id_2)]) > 0:\n",
    "    actual_rating_2 = df_final[(df_final['user_id'] == user_id_2) & (df_final['prod_id'] == prod_id_2)]['rating'].values[0]\n",
    "    print(f\"Actual rating for User {user_id_2} and Product {prod_id_2}: {actual_rating_2}\")\n",
    "else:\n",
    "    actual_rating_2 = None\n",
    "    print(f\"User {user_id_2} has not interacted with Product {prod_id_2} yet.\")\n",
    "\n",
    "# Step 7: Print the predicted rating\n",
    "print(f\"Predicted rating for User {user_id_2} and Product {prod_id_2}: {prediction_optimized_2.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your observations here:**\n",
    "\n",
    "The interacted product is better with the optimized model than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDlNB7tnTsnu"
   },
   "source": [
    "### **Identifying similar items to a given item (nearest neighbors)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLdDiFA6Tsnu"
   },
   "source": [
    "We can also find out **similar items** to a given item or its nearest neighbors based on this **KNNBasic algorithm**. Below we are finding the 5 most similar items to the item with internal id 0 based on the `msd` distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "ZRJS4oDFTsnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "5 most similar items to item with internal id 0:\n",
      "[14, 75, 120, 164, 180]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the rating dataset\n",
    "reader = Reader(rating_scale=(1, 5))  # Rating scale (1-5)\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Declare the similarity options for item-item similarity (using MSD)\n",
    "sim_options = {\n",
    "    'name': 'msd',  # Mean Squared Difference (MSD) similarity measure\n",
    "    'user_based': False  # Item-based collaborative filtering\n",
    "}\n",
    "\n",
    "# Step 4: Initialize the KNNBasic algorithm with item-item similarity using MSD\n",
    "model_item_item_msd = KNNBasic(sim_options=sim_options, random_state=1)\n",
    "\n",
    "# Step 5: Train the model on the training set\n",
    "model_item_item_msd.fit(trainset)\n",
    "\n",
    "# Step 6: Find the 5 most similar items to the item with internal id 0\n",
    "# Here we are using the item with internal id 0. Adjust this if you need another item.\n",
    "internal_id = 0\n",
    "neighbors = model_item_item_msd.get_neighbors(internal_id, k=5)\n",
    "\n",
    "# Step 7: Print the most similar items (item ids of the nearest neighbors)\n",
    "print(f\"5 most similar items to item with internal id {internal_id}:\")\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting top 5 products for userId = \"A1A5KUIIIHFF4U\" with similarity based recommendation system.**\n",
    "\n",
    "**Hint:** Use the get_recommendations() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzoEbuZFTsnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products for user A1A5KUIIIHFF4U using SVD:\n",
      "Product ID: B003L1ZYZ6, Predicted Rating: 4.506613716440301\n",
      "Product ID: B001TH7T2U, Predicted Rating: 4.453977045421942\n",
      "Product ID: B001TH7GUU, Predicted Rating: 4.431997333958276\n",
      "Product ID: B003UH0Z9Q, Predicted Rating: 4.41636140802659\n",
      "Product ID: B001UI2FPE, Predicted Rating: 4.393831224976366\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_final[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train the model using SVD\n",
    "model_svd = SVD(n_factors=50, reg_all=0.02, lr_all=0.005)\n",
    "model_svd.fit(trainset)\n",
    "\n",
    "# Make predictions\n",
    "user_id = 'A1A5KUIIIHFF4U'\n",
    "top_n = 5\n",
    "\n",
    "def get_recommendations_svd(data, user_id, model, top_n=5):\n",
    "    recommendations = []\n",
    "    \n",
    "    # Get a list of products that the user hasn't rated\n",
    "    user_ratings = data.df[data.df['user_id'] == user_id]['prod_id']\n",
    "    all_products = data.df['prod_id'].unique()\n",
    "    non_interacted_products = [prod for prod in all_products if prod not in user_ratings.values]\n",
    "    \n",
    "    # Predict ratings for the non-interacted products\n",
    "    for product in non_interacted_products:\n",
    "        est = model.predict(user_id, product).est\n",
    "        recommendations.append((product, est))\n",
    "    \n",
    "    # Sort by predicted rating (highest first)\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return recommendations[:top_n]\n",
    "\n",
    "# Get top 5 recommended products for the user\n",
    "top_recommendations_svd = get_recommendations_svd(data, user_id, model_svd, top_n)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Top 5 recommended products for user {user_id} using SVD:\")\n",
    "for prod_id, predicted_rating in top_recommendations_svd:\n",
    "    print(f\"Product ID: {prod_id}, Predicted Rating: {predicted_rating}\")\n",
    "\n",
    "# I was only getting identical rating values for so many ungodly hours of working on this. I don't really know what I did this time that was right, but it finally worked. Hurrah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "_kXVTiysTsnv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products with their predicted ratings:\n",
      "      prod_id  predicted_ratings\n",
      "0  B003L1ZYZ6           4.506614\n",
      "1  B001TH7T2U           4.453977\n",
      "2  B001TH7GUU           4.431997\n",
      "3  B003UH0Z9Q           4.416361\n",
      "4  B001UI2FPE           4.393831\n"
     ]
    }
   ],
   "source": [
    "# Building the dataframe for above recommendations with columns \"prod_id\" and \"predicted_ratings\"\n",
    "# Step 1: Get the top 5 recommended products for the user (using previously defined get_recommendations_svd function)\n",
    "top_recommendations_svd = get_recommendations_svd(data, user_id, model_svd, top_n)\n",
    "\n",
    "# Step 2: Create a DataFrame with columns 'prod_id' and 'predicted_ratings'\n",
    "df_recommendations = pd.DataFrame(top_recommendations_svd, columns=[\"prod_id\", \"predicted_ratings\"])\n",
    "\n",
    "# Step 3: Display the DataFrame\n",
    "print(\"Top 5 recommended products with their predicted ratings:\")\n",
    "print(df_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHzmYvs0Tsnv"
   },
   "source": [
    "Now as we have seen **similarity-based collaborative filtering algorithms**, let us now get into **model-based collaborative filtering algorithms**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKgJpSA9vOOL"
   },
   "source": [
    "### **Model 3: Model-Based Collaborative Filtering - Matrix Factorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YF6ZGyqhCAob"
   },
   "source": [
    "Model-based Collaborative Filtering is a **personalized recommendation system**, the recommendations are based on the past behavior of the user and it is not dependent on any additional information. We use **latent features** to find recommendations for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4Otha8ovOOL"
   },
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sGl3QkLvOOL"
   },
   "source": [
    "SVD is used to **compute the latent features** from the **user-item matrix**. But SVD does not work when we **miss values** in the **user-item matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07-2PT5Ssjqm"
   },
   "outputs": [],
   "source": [
    "# Using SVD matrix factorization. Use random_state = 1\n",
    "\n",
    "# Training the algorithm on the trainset\n",
    "\n",
    "# Use the function precision_recall_at_k to compute precision@k, recall@k, F1-Score, and RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ6fTuCDnVNL"
   },
   "source": [
    "**Write your observations here:___________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now predict the rating for a user with `userId = \"A3LDPF5FMB782Z\"` and `prod_id = \"1400501466`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWIhfdxXsjqm"
   },
   "outputs": [],
   "source": [
    "# Making prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIjzqDY5nVNM"
   },
   "source": [
    "**Write your observations here:___________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1aYxVeMnVNM"
   },
   "source": [
    "**Below we are predicting rating for the `userId = \"A2UOHALGF2X77Q\"` and `productId = \"1400501466\"`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APm-uMSvcAMf"
   },
   "outputs": [],
   "source": [
    "# Making prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEL6dy3wnVNM"
   },
   "source": [
    "**Write your observations here:___________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x13Eb9Owvpcw"
   },
   "source": [
    "### **Improving Matrix Factorization based recommendation system by tuning its hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQcDPhhcnVNN"
   },
   "source": [
    "Below we will be tuning only three hyperparameters:\n",
    "- **n_epochs**: The number of iterations of the SGD algorithm.\n",
    "- **lr_all**: The learning rate for all parameters.\n",
    "- **reg_all**: The regularization term for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bM81V_hvtwv"
   },
   "outputs": [],
   "source": [
    "# Set the parameter space to tune\n",
    "\n",
    "# Performing 3-fold gridsearch cross-validation\n",
    "\n",
    "# Fitting data\n",
    "\n",
    "# Best RMSE score\n",
    "\n",
    "# Combination of parameters that gave the best RMSE score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzY78HsrnVNO"
   },
   "source": [
    "Now, we will **the build final model** by using **tuned values** of the hyperparameters, which we received using grid search cross-validation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA_7xe-nnhuu"
   },
   "outputs": [],
   "source": [
    "# Build the optimized SVD model using optimal hyperparameter search. Use random_state=1\n",
    "\n",
    "# Train the algorithm on the trainset\n",
    "\n",
    "# Use the function precision_recall_at_k to compute precision@k, recall@k, F1-Score, and RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HJvPsjITsny"
   },
   "source": [
    "**Write your observations here:_____________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Steps:**\n",
    "- **Predict rating for the user with `userId=\"A3LDPF5FMB782Z\"`, and `prod_id= \"1400501466\"` using the optimized model**\n",
    "- **Predict rating for `userId=\"A2UOHALGF2X77Q\"` who has not interacted with `prod_id =\"1400501466\"`, by using the optimized model**\n",
    "- **Compare the output with the output from the baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use svd_algo_optimized model to recommend for userId \"A3LDPF5FMB782Z\" and productId \"1400501466\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use svd_algo_optimized model to recommend for userId \"A2UOHALGF2X77Q\" and productId \"1400501466\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnwPwgjB8DwS"
   },
   "source": [
    "### **Conclusion and Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuqnifw9NF2p"
   },
   "source": [
    "**Write your conclusion and recommendations here**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
